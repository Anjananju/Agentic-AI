Automated Blog Writer â€” Multi-Agent LLM System
Kaggle AI Agents Capstone Project Submission
This project implements a multi-agent AI system that generates fully structured, edited, SEO-optimized blog articles from a topic prompt.
It showcases all required AI Agent Capstone features:
 Multi-agent orchestration
â€” Research Agent
â€” Outline Agent
â€” Draft Agent
â€” Editor Agent
â€” SEO Agent
â€” Supervisor Agent (Coordinator + Parallel Execution)
Tool usage
â€” Custom WebScraperTool for URL-based content extraction
â€” Optional custom HTTP LLM endpoint
â€” Built-in LLM provider abstraction (HuggingFace / Custom APIs)
Session & Memory
â€” InMemorySessionService
â€” Persistent MemoryBank (JSON-backed)
Observability
â€” Structured logging
â€” Execution trace by job_id
â€” Supports pause/resume

Project Overview
The Automated Blog Writer Agent takes:
A topic
A target audience
Optional reference URLs
Then creates a complete blog article through a multi-step LLM pipeline:
Agent Pipeline
ResearchAgent â†’ Fetches & summarizes reference URLs
OutlineAgent â†’ Builds a JSON-based article outline
DraftAgent â†’ Expands all sections in parallel
EditorAgent â†’ Cleans & rewrites each section
SEOAgent â†’ Produces SEO metadata
SupervisorAgent â†’ Runs, monitors, pauses, resumes the job

Architecture Diagram
User Input
   â”‚
   â–¼
SupervisorAgent
   â”‚
   â”œâ”€â”€ ResearchAgent  â†’  WebScraperTool
   â”œâ”€â”€ OutlineAgent
   â”œâ”€â”€ DraftAgent  (Parallel threads)
   â”œâ”€â”€ EditorAgent
   â””â”€â”€ SEOAgent
   â”‚
   â–¼
 Final Article (Markdown + SEO)


 Repository Structure
src/
â”‚
â”œâ”€â”€ llm_provider.py
â”œâ”€â”€ demo_app.py
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ supervisor_agent.py
â”‚   â”œâ”€â”€ research_agent.py
â”‚   â”œâ”€â”€ outline_agent.py
â”‚   â”œâ”€â”€ draft_agent.py
â”‚   â”œâ”€â”€ editor_agent.py
â”‚   â””â”€â”€ seo_agent.py
â”‚
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ webscraper_tool.py
â”‚
â”œâ”€â”€ session/
â”‚   â””â”€â”€ in_memory_session.py
â”‚
â”œâ”€â”€ memory/
â”‚   â””â”€â”€ memory_bank.py
â”‚
â””â”€â”€ observability/
    â””â”€â”€ logger.py

requirements.txt
README.md


ğŸ› ï¸ Installation
Local Setup
git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git
cd YOUR_REPO

python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt
Run the demo UI:
streamlit run src/demo_app.py


ğŸ§© Running the Agent
Here is a minimal example:
from llm_provider import HuggingFaceProvider
from agents.supervisor_agent import SupervisorAgent

llm = HuggingFaceProvider(model_name="gpt2")  # or your own HF model
agent = SupervisorAgent(llm)

result = agent.start_job("Future of AI Agents", "Developers")
print(result["content"])


ğŸ¤– HuggingFace / Custom LLM Support
You can use:
HuggingFace local models
HuggingFace Inference API
Custom HTTP LLM endpoint
Any other model by creating a new provider class
Example (HTTP endpoint):
from llm_provider import HTTPProvider
llm = HTTPProvider(endpoint="https://your-model-endpoint")


ğŸ§ª Parallelism
The system uses a ThreadPoolExecutor to draft sections simultaneously:
DraftAgent.expand(...)  â† executed in parallel workers
EditorAgent.run(...)
Adjust workers:
SupervisorAgent(llm, workers=4)

ğŸ’¾ Memory & Session
Short-term session:
InMemorySessionService stores job snapshots
Long-term memory:
MemoryBank stores user tone profile or reusable data
ğŸ‘€ Observability
Every step logs:
Agent name
Timestamp
Job ID
Status
Logs appear automatically in the console/Streamlit.


ğŸ§® Pause/Resume
agent.pause_job(job_id)
agent.resume_job(job_id)
ğŸ§ª Requirements
streamlit
requests
beautifulsoup4
transformers
torch
